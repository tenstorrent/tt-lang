# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0

"""
Tests for two sequential compute operations in the same kernel.

This tests the DST register synchronization when multiple ttl.compute ops
are lowered to separate scf.for loop nests, each with their own sync ops.

Two test patterns:
1. TestTwoComputesChained: compute1(a + b) -> r0, then compute2(r0 * r0) -> result
   Uses intermediate CBs. Currently compile-only (device execution requires runner changes).

2. TestTwoComputesSecondResult: compute1(a + b) -> CB2, compute2(a * b) -> CB2
   Uses buffer_factor=2 and custom writer to discard first result and write second.

TODO(intermediate-cb-support): Once the compiler/runner supports intermediate CBs:
  - TestTwoComputesChained: Enable device execution tests (remove skip markers)
  - TestTwoComputesSecondResult: May be able to use standard writer if compiler
    auto-generates inter-loop CB sync (cb_wait/cb_pop between compute loops)
"""

from typing import Tuple

import pytest
import torch
from torch import Tensor
from ttmlir.ir import Context, Module

from ..base import ME2ETestBase
from ..config import E2EConfig
from ..builder.dtype_utils import torch_dtype_to_mlir_str
from ..builder.thread_builder import generate_layout_attrs
from ..builder.dm_builder import DMThreadBuilder

import ttl.dialects.ttl as ttl


class TwoComputeTestBase(ME2ETestBase):
    """
    Base class for two-compute operation tests.

    Tests DST register synchronization when multiple ttl.compute ops
    are lowered to separate scf.for loop nests.
    """

    # Override in subclasses.
    OP_NAME: str
    ARITY: int = 2
    INPUT_SHAPE: Tuple[int, int] = (1, 1)
    INPUT_DTYPE: torch.dtype = torch.bfloat16
    INPUT_RANGE: Tuple[float, float] = (-1.0, 1.0)

    @pytest.fixture(scope="class")
    def config(self) -> E2EConfig:
        """Get test configuration."""
        return E2EConfig(grid_shape=self.INPUT_SHAPE, dtype=self.INPUT_DTYPE)

    def torch_reference(self, *inputs: Tensor) -> Tensor:
        """
        Compute golden output using torch.

        Override in subclasses.
        """
        raise NotImplementedError("Subclasses must implement torch_reference()")

    def get_mlir_template(self, config: E2EConfig) -> str:
        """
        Return MLIR string template for the two-compute operation.

        Override in subclasses.
        """
        raise NotImplementedError("Subclasses must implement get_mlir_template()")

    @pytest.mark.order(1)
    def test_build_module(self, config: E2EConfig) -> None:
        """Build TTL module for two-compute test from MLIR template."""
        # Generate random inputs.
        lo, hi = self.INPUT_RANGE
        torch_inputs = []
        for _ in range(self.ARITY):
            t = torch.rand(config.tensor_shape, dtype=config.dtype) * (hi - lo) + lo
            torch_inputs.append(t)

        # Parse MLIR template.
        mlir_str = self.get_mlir_template(config)

        ctx = Context()
        ttl.ensure_dialects_registered(ctx)
        with ctx:
            module = Module.parse(mlir_str, ctx)
            module.operation.verify()

        # Save module to file for subsequent stages.
        module_file = self.output_file("module.mlir")
        with open(module_file, "w") as f:
            f.write(str(module))

        # Save inputs for golden comparison.
        torch.save(torch_inputs, self.output_file("inputs.pt"))

        # Compute and save golden output.
        golden = self.torch_reference(*torch_inputs)
        torch.save(golden, self.output_file("golden.pt"))

    @pytest.mark.order(2)
    def test_compile_to_ttkernel(self) -> None:
        """Run TTL-to-TTKernel pass pipeline on the generated module."""
        super().test_compile_to_ttkernel()

    @pytest.mark.order(3)
    def test_translate_to_cpp(self) -> None:
        """Translate two-compute TTKernel to C++ kernels."""
        super().test_translate_to_cpp()


class TestTwoComputesChained(TwoComputeTestBase):
    """
    Test two sequential compute ops with intermediate CBs.

    Pattern: compute1(a + b) -> r0, then compute2(r0 * r0) -> result

    The result is (a + b)^2.

    This test validates:
    1. Each compute op gets its own DST sync ops (acquire/commit/wait/release)
    2. The intermediate CB (CB3) passes data between computes
    3. The second compute correctly reads from the intermediate CB

    Note: Device execution requires runner changes for intermediate CBs,
    so this test is compile-only.

    TODO(intermediate-cb-support): Once runner supports intermediate CBs:
      - Remove skip markers from test_execute and test_validate_golden
      - The runner needs to allocate CBs 2 and 3 for intermediate data
      - May need to add inter-loop CB sync (cb_push_back after first loop,
        cb_wait_front before second loop) if not auto-generated by compiler
    """

    OP_NAME = "two_computes_chained"
    ARITY = 2

    def torch_reference(self, a: Tensor, b: Tensor) -> Tensor:
        """Compute (a + b)^2."""
        return (a + b) ** 2

    def get_mlir_template(self, config: E2EConfig) -> str:
        """
        Build MLIR with two sequential ttl.compute regions.

        Pattern:
          CB0, CB1 -> compute1(add) -> CB2 (intermediate)
          CB3 -> compute2(mul) -> CB4 (output)

        Uses 5 CBs: 2 inputs, 1 intermediate output, 1 intermediate input, 1 final output.
        """
        rows, cols = config.grid_shape
        dtype = torch_dtype_to_mlir_str(config.dtype)
        bf = config.buffer_factor

        layout_attrs = generate_layout_attrs(config)
        dm_builder = DMThreadBuilder(config)

        # Reader for 2 inputs, writer for CB4 (final output).
        reader_mlir = dm_builder.build_reader(num_inputs=2, total_cbs=5)
        writer_mlir = dm_builder.build_writer(output_cbs=[4], total_cbs=5)

        compute_mlir = f"""
  func.func @compute_two_ops(%a: tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>,
                             %b: tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
      -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      attributes {{ttl.kernel_thread = #ttkernel.thread<compute>}} {{
    %init = tensor.empty() : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    %cb0 = ttl.bind_cb {{cb_index = 0, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
    %cb1 = ttl.bind_cb {{cb_index = 1, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
    %cb2 = ttl.bind_cb {{cb_index = 2, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>

    %a_ready = ttl.cb_wait %cb0 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
    %b_ready = ttl.cb_wait %cb1 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
    %init_cb = ttl.attach_cb %init, %cb2 : (tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>, !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>) -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // First compute: a + b -> r0
    %r0 = ttl.compute
        ins(%a_ready, %b_ready : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>,
                                 tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        outs(%init_cb : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        {{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>],
         iterator_types = ["parallel", "parallel"]}} {{
    ^bb0(%a_tile: !ttcore.tile<32x32, {dtype}>,
         %b_tile: !ttcore.tile<32x32, {dtype}>,
         %out_tile: !ttcore.tile<32x32, {dtype}>):
      %sum = ttl.tile_add %a_tile, %b_tile : !ttcore.tile<32x32, {dtype}>
      %view0 = ttl.cb_reserve %cb2 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.store %sum, %view0 : !ttcore.tile<32x32, {dtype}>, tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.cb_push %cb2 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
      ttl.yield %sum : !ttcore.tile<32x32, {dtype}>
    }} -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // Attach r0 to CB3 for second compute input.
    %cb3 = ttl.bind_cb {{cb_index = 3, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
    %r0_cb = ttl.attach_cb %r0, %cb3 : (tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>, !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>) -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // New init for second compute output.
    %init2 = tensor.empty() : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
    %cb4 = ttl.bind_cb {{cb_index = 4, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
    %init2_cb = ttl.attach_cb %init2, %cb4 : (tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>, !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>) -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // Second compute: r0 * r0 -> result
    %result = ttl.compute
        ins(%r0_cb, %r0_cb : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>,
                             tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        outs(%init2_cb : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        {{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>],
         iterator_types = ["parallel", "parallel"]}} {{
    ^bb0(%r0_tile1: !ttcore.tile<32x32, {dtype}>,
         %r0_tile2: !ttcore.tile<32x32, {dtype}>,
         %out_tile: !ttcore.tile<32x32, {dtype}>):
      %product = ttl.tile_mul %r0_tile1, %r0_tile2 : !ttcore.tile<32x32, {dtype}>
      %view1 = ttl.cb_reserve %cb4 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.store %product, %view1 : !ttcore.tile<32x32, {dtype}>, tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.cb_push %cb4 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
      ttl.yield %product : !ttcore.tile<32x32, {dtype}>
    }} -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    func.return %result : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
  }}
"""

        return f"""{layout_attrs}

module {{
{reader_mlir}
{compute_mlir}
{writer_mlir}
}}
"""

    @pytest.mark.order(3)
    def test_translate_to_cpp(self) -> None:
        """Translate to C++ and verify two sets of DST sync ops."""
        super().test_translate_to_cpp()

        # Additional verification: check for two sets of DST sync ops.
        # Find the compute kernel file in the kernels directory.
        kernel_dir = self.OUTPUT_DIR / "kernels"
        compute_files = list(kernel_dir.glob("compute*.cpp"))
        assert compute_files, "No compute kernel file found"
        cpp_file = compute_files[0]
        with open(cpp_file) as f:
            source = f.read()

        assert (
            source.count("tile_regs_acquire()") == 2
        ), "Should have 2 tile_regs_acquire (one per compute)"
        assert (
            source.count("tile_regs_release()") == 2
        ), "Should have 2 tile_regs_release (one per compute)"
        assert (
            source.count("add_binary_tile_init()") == 1
        ), "Should have 1 add operation (first compute)"
        assert (
            source.count("mul_binary_tile_init()") == 1
        ), "Should have 1 mul operation (second compute)"

    @pytest.mark.order(4)
    @pytest.mark.skip(
        reason="Device execution requires runner changes for intermediate CBs"
    )
    def test_execute(self, device) -> None:
        """Skip device execution - requires runner changes for intermediate CBs."""
        pass

    @pytest.mark.order(5)
    @pytest.mark.skip(reason="No device execution, so no result to validate")
    def test_validate_golden(self) -> None:
        """Skip validation - no device result available."""
        pass


class TestTwoComputesSecondResult(TwoComputeTestBase):
    """
    Test two sequential compute ops with proper inter-loop CB sync.

    Pattern: compute1(a + b) -> CB2, compute2(a * b) -> CB2
    Writer: discards first result, writes second result to DRAM.

    The final result is a * b (second compute's output).

    This test validates:
    1. Each compute op gets its own DST sync ops
    2. Using buffer_factor=2 allows CB to hold both results
    3. Writer can skip first result with cb_wait/cb_pop before reading second
    4. The second compute's result is correctly written (a * b)

    TODO(compiler-inter-loop-sync): Once compiler auto-generates inter-loop CB sync:
      - Replace custom writer with standard DMThreadBuilder.build_writer()
      - The compiler should insert cb_pop after first compute loop so writer
        only sees the second result
      - Can then use buffer_factor=1 since only one result needs to be in CB
    """

    OP_NAME = "two_computes_second_result"
    ARITY = 2

    def torch_reference(self, a: Tensor, b: Tensor) -> Tensor:
        """Compute a * b (second compute's output)."""
        return a * b

    def get_mlir_template(self, config: E2EConfig) -> str:
        """
        Build MLIR with two computes and a custom writer that skips first result.

        Uses buffer_factor=2 so CB2 can hold both results.
        Custom writer: cb_wait -> cb_pop (discard first) -> cb_wait -> write -> cb_pop
        """
        rows, cols = config.grid_shape
        dtype = torch_dtype_to_mlir_str(config.dtype)
        # Use buffer_factor=2 so CB can hold both compute results
        bf = 2

        layout_attrs = generate_layout_attrs(config)
        dm_builder = DMThreadBuilder(config)

        reader_mlir = dm_builder.build_reader(num_inputs=2, total_cbs=3)

        # Custom writer that discards first result and writes second
        # Note: DRAM tensor uses #layout attribute and 2D shape [tiles_y, tiles_x]
        # IMPORTANT: We must use attach_cb on the wait result to prevent it from being
        # optimized away. The compiler will otherwise reorder cb_pop before cb_wait.
        writer_mlir = f"""
// Custom writer: discards first result, writes second result to DRAM.
func.func @writer(%out0: tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>, #layout>)
    attributes {{ttl.base_cta_index = 3 : i32, ttl.crta_indices = [2 : i32], ttl.kernel_thread = #ttkernel.thread<noc>}} {{
  %cb_out = ttl.bind_cb {{cb_index = 2, buffer_factor = {bf}}} : !ttl.cb<[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}>

  // WORKAROUND: Manually discard first result because compiler doesn't yet
  // insert inter-loop CB sync. Once TTLInsertInterLoopCBSync pass exists,
  // this test can use a standard writer and buffer_factor=1.
  %discard = ttl.cb_wait %cb_out : <[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<1x1x!ttcore.tile<32x32, {dtype}>>
  ttl.cb_pop %cb_out : <[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}>

  // Now read and write the second result (a * b).
  %wait_out = ttl.cb_wait %cb_out : <[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<1x1x!ttcore.tile<32x32, {dtype}>>
  %attached_out = ttl.attach_cb %wait_out, %cb_out : (tensor<1x1x!ttcore.tile<32x32, {dtype}>>, !ttl.cb<[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}>) -> tensor<1x1x!ttcore.tile<32x32, {dtype}>>
  %c0 = arith.constant 0 : index
  %slice_out = ttl.tensor_slice %out0[%c0, %c0] : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>, #layout> -> tensor<1x1x!ttcore.tile<32x32, {dtype}>, #layout>
  %xf_out = ttl.copy %cb_out, %slice_out : (!ttl.cb<[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}>, tensor<1x1x!ttcore.tile<32x32, {dtype}>, #layout>) -> !ttl.transfer_handle<write>
  ttl.wait %xf_out : !ttl.transfer_handle<write>
  ttl.cb_pop %cb_out : <[1, 1], !ttcore.tile<32x32, {dtype}>, {bf}>

  func.return
}}
"""

        compute_mlir = f"""
  func.func @compute_two_ops_second_result(
      %a: tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>,
      %b: tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
      -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      attributes {{ttl.kernel_thread = #ttkernel.thread<compute>}} {{
    %init = tensor.empty() : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    %cb0 = ttl.bind_cb {{cb_index = 0, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
    %cb1 = ttl.bind_cb {{cb_index = 1, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
    %cb2 = ttl.bind_cb {{cb_index = 2, buffer_factor = {bf}}} : !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>

    %a_ready = ttl.cb_wait %cb0 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
    %b_ready = ttl.cb_wait %cb1 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
    %init_cb = ttl.attach_cb %init, %cb2 : (tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>, !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>) -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // First compute: a + b -> CB2 (will be discarded by writer)
    %r0 = ttl.compute
        ins(%a_ready, %b_ready : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>,
                                 tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        outs(%init_cb : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        {{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>],
         iterator_types = ["parallel", "parallel"]}} {{
    ^bb0(%a_tile: !ttcore.tile<32x32, {dtype}>,
         %b_tile: !ttcore.tile<32x32, {dtype}>,
         %out_tile: !ttcore.tile<32x32, {dtype}>):
      %sum = ttl.tile_add %a_tile, %b_tile : !ttcore.tile<32x32, {dtype}>
      %view0 = ttl.cb_reserve %cb2 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.store %sum, %view0 : !ttcore.tile<32x32, {dtype}>, tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.cb_push %cb2 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
      ttl.yield %sum : !ttcore.tile<32x32, {dtype}>
    }} -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // Re-attach CB2 for second compute output.
    %out2_cb = ttl.attach_cb %r0, %cb2 : (tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>, !ttl.cb<[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>) -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    // Second compute: a * b -> CB2 (writer reads this one)
    %result = ttl.compute
        ins(%a_ready, %b_ready : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>,
                                 tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        outs(%out2_cb : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>)
        {{indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>,
                          affine_map<(d0, d1) -> (d0, d1)>],
         iterator_types = ["parallel", "parallel"]}} {{
    ^bb0(%a_tile2: !ttcore.tile<32x32, {dtype}>,
         %b_tile2: !ttcore.tile<32x32, {dtype}>,
         %out_tile2: !ttcore.tile<32x32, {dtype}>):
      %product = ttl.tile_mul %a_tile2, %b_tile2 : !ttcore.tile<32x32, {dtype}>
      %view1 = ttl.cb_reserve %cb2 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}> -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.store %product, %view1 : !ttcore.tile<32x32, {dtype}>, tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
      ttl.cb_push %cb2 : <[{rows}, {cols}], !ttcore.tile<32x32, {dtype}>, {bf}>
      ttl.yield %product : !ttcore.tile<32x32, {dtype}>
    }} -> tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>

    func.return %result : tensor<{rows}x{cols}x!ttcore.tile<32x32, {dtype}>>
  }}
"""

        return f"""{layout_attrs}

module {{
{reader_mlir}
{compute_mlir}
{writer_mlir}
}}
"""

    @pytest.mark.order(4)
    @pytest.mark.requires_device
    def test_execute(self, device) -> None:
        """Execute on device."""
        super().test_execute(device)

    @pytest.mark.order(5)
    def test_validate_golden(self) -> None:
        """Validate result against golden (a * b)."""
        super().test_validate_golden()
