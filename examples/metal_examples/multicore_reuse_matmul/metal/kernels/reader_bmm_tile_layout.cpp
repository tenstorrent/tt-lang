// SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.
//
// SPDX-License-Identifier: Apache-2.0

#include "api/dataflow/dataflow_api.h"
#include <stdint.h>

void kernel_main() {
  // a tensor args
  uint32_t a_tensor_addr = get_arg_val<uint32_t>(0);
  uint32_t a_tensor_start_tile_id = get_arg_val<uint32_t>(1);
  uint32_t a_tensor_stride_w = get_arg_val<uint32_t>(2);
  uint32_t a_tensor_stride_h = get_arg_val<uint32_t>(3);
  uint32_t a_tensor_next_block_stride = get_arg_val<uint32_t>(4);

  // a block args
  uint32_t a_block_w = get_arg_val<uint32_t>(5);
  uint32_t a_block_h = get_arg_val<uint32_t>(6);
  uint32_t a_block_num_tiles = get_arg_val<uint32_t>(7);

  // b tensor args
  uint32_t b_tensor_addr = get_arg_val<uint32_t>(8);
  uint32_t b_tensor_start_tile_id = get_arg_val<uint32_t>(9);
  uint32_t b_tensor_stride_w = get_arg_val<uint32_t>(10);
  uint32_t b_tensor_stride_h = get_arg_val<uint32_t>(11);
  uint32_t b_tensor_next_block_stride = get_arg_val<uint32_t>(12);

  // b block args
  uint32_t b_block_w = get_arg_val<uint32_t>(13);
  uint32_t b_block_h = get_arg_val<uint32_t>(14);
  uint32_t b_block_num_tiles = get_arg_val<uint32_t>(15);

  // a/b common args
  uint32_t num_blocks = get_arg_val<uint32_t>(16);

  constexpr uint32_t cb_id_a = 0;
  constexpr uint32_t cb_id_b = 1;

  const uint32_t a_single_tile_size_bytes = get_tile_size(cb_id_a);
  const uint32_t b_single_tile_size_bytes = get_tile_size(cb_id_b);

  uint32_t l1_write_addr_a;
  uint32_t l1_write_addr_b;

  constexpr auto s0_args = TensorAccessorArgs<0>();
  const auto s0 =
      TensorAccessor(s0_args, a_tensor_addr, a_single_tile_size_bytes);
  constexpr auto s1_args =
      TensorAccessorArgs<s0_args.next_compile_time_args_offset()>();
  const auto s1 =
      TensorAccessor(s1_args, b_tensor_addr, b_single_tile_size_bytes);

  uint32_t a_tensor_current_block_start_tile_id = a_tensor_start_tile_id;
  uint32_t b_tensor_current_block_start_tile_id = b_tensor_start_tile_id;
  for (uint32_t block = 0; block < num_blocks; block++) {
    cb_reserve_back(cb_id_a, a_block_num_tiles);
    cb_reserve_back(cb_id_b, b_block_num_tiles);

    l1_write_addr_a = get_write_ptr(cb_id_a);
    l1_write_addr_b = get_write_ptr(cb_id_b);
    // subblocking to be generated by compiler
    uint32_t a_tensor_row_start_tile_id = a_tensor_current_block_start_tile_id;
    for (uint32_t h = 0; h < a_block_h; h++) {
      uint32_t a_tensor_tile_id = a_tensor_row_start_tile_id;
      for (uint32_t w = 0; w < a_block_w; w++) {
        noc_async_read_tile(a_tensor_tile_id, s0, l1_write_addr_a);
        l1_write_addr_a += a_single_tile_size_bytes;
        a_tensor_tile_id += a_tensor_stride_w;
      }
      a_tensor_row_start_tile_id += a_tensor_stride_h;
    }
    a_tensor_current_block_start_tile_id += a_tensor_next_block_stride;

    uint32_t b_tensor_row_start_tile_id = b_tensor_current_block_start_tile_id;
    for (uint32_t h = 0; h < b_block_h; h++) {
      uint32_t b_tensor_tile_id = b_tensor_row_start_tile_id;
      for (uint32_t w = 0; w < b_block_w; w++) {
        noc_async_read_tile(b_tensor_tile_id, s1, l1_write_addr_b);
        l1_write_addr_b += b_single_tile_size_bytes;
        b_tensor_tile_id += b_tensor_stride_w;
      }
      b_tensor_row_start_tile_id += b_tensor_stride_h;
    }
    b_tensor_current_block_start_tile_id += b_tensor_next_block_stride;
    // end of subblocking

    noc_async_read_barrier();

    cb_push_back(cb_id_a, a_block_num_tiles);
    cb_push_back(cb_id_b, b_block_num_tiles);
  }
}
